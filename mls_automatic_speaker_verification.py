# -*- coding: utf-8 -*-
"""MiniProjectReport.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zbCPZ8xXQqwqZCkdO8e2c4Yov_7gs1bV

# Project Report

### Title: "Automatic Speaker Verification with Siamese Neural Networks"
### Student name: "Juuso Hämäläinen"

## Abstract

_In this project the goal was to generate Automatic Speaker Verification (AVS) system with Deep Neural Networks. For this purpose a Deep Convolutional Neural Network was created as the base model, which was then utilized as a part of Siamese Neural Network. Four different Siamese Neural Network models were trained to simulate different situations in speaker verification. Then the models were evaluated accordingly to provide information about how different scenarios affect the learning efficiency and prediction accuracy of the model._

_Results indicate that with proper training more generalized Models could be proven semi-useful and transferable, thus this project did not provide too relevant results besides all efforts._

## Introduction

Describe the research question:
* What is your hypothesis?

> The hypothesis for this project is, that automatic speaker verification Models reliability, accuracy and and transferability are is highly dependent on the data used for the training and testing of the model and training Model with too biased data (or for too narrow scenarios), will have huge effect on the overall reliability of the Model.

* What are the main issues are you tackling?

> Main issues in this project are related to the inreliability of the Siamese Neural Networks 

* Break down your approach into steps and describe them. 

>Approach in this project consists on following steps:

>1. Split the Data into four different Train-and Test Sets. Each including a different scenario. Scenarios are following: Basic-scenario, Text dependent scenario, text limited scenario, and text independent scenario. 

>2. Build Base-line Deep Convolutional Neural Network Model and Siamese Model based on the Base Model.

>3. Train different Models with data correlating to different scenarios.

>4. Evaluate Models with the Test Sets made for the scenario the Model was trained for.

>5. Evaluate Models with Test Sets from other scenarios, than which the Model was trained for to see how transferable the Model would be and how much effect the training scenario has to transferability.

Brief description of which dataset you are using.

>In this project the AudioMNIST data set is used for training and testing the generated Models. AudioMNIST consists of 30000 utterances of spoken digits splitted evenly among 60 different speakers.

How many different ways (approaches) you know that could be used to resolve the hypothesis? 

>It is notable, that there are many other ways to resolve the hypothesis, which might be useful to either try or even append to this project if further research is to be done. One worth mentioning is to use transfer learning, to utilize other data sets and/or use the Models generated and trained on this project with testing data out of the context, which they were trained on to see better the overall effects how the training data affects the predictions / evaluations of the Models.

>Totally different approaches could be also used for training and testing the Models, like using a mel-spectrogram images and 2D Convolutional networks, or as provided in this paper (https://arxiv.org/pdf/1812.00271.pdf) to use chunks of MFCCs and mutual information calculations based on them with Deep Neural Network and using SincNet as proved in the same paper.

>Also i-vertor, random forests and other approaches can also be utilized for this kind of a classification task.

Why did you choose your specific approach?

>This specific approach was chosen, because training four* different models and then evaluating them on the Test Set specified for the scenario and then evaluating the Model against Test Set from totally different scenario, would possibly provide information on which type of a training scenario would yield the most error tolerant / transferable Models.

>>*(eight in total, if the Models trained with the other sampling scenario are included)

What are the other methods' merits or drawbacks, and what are the advantages of your method?

>The merits are, that the Siamese Deep Neural Network can be efficient and good way for running test simulations on binary classification task and it can provide good base-line for evaluating Models in transfer learning. 

>The drawbacks are, that as with all Neural Networks, finding the _goldilocks_ values for parameters are hard without using gridsearches etc, and also finding the optimal Model structure (the types and numbers of layers and activations to be utilize), is quite chalenging and time taking.

>Other drawbacks are that as Neural Networks are usually bit of a black boxes and here we are trying to achieve a Model, which could be applicable for multiple different classification scenarios, finding the correct structure and values becomes even more difficult.

What are the goals you achieved in this project?
>Result wise this project has yield so far results, which indicate that Siamese Neural Networks can be utilized to make classifiers for speech data using MFCC-features. Yet the results have been slightly hars, due to lack of time spend optimizing and training the networks and due to little bit too biased training scenario, which lead to samples being too often providing same outcome (not same speakers). A way to compensate this was provided in this project, but it yielded not too great results either if the goal was to produce accurate ASV-system. 

> Even tho the training setup was biased, it was most useful for learning experience, as it provided insight on how the proper training of the Models should have been done, and gave a good look at the tools used to reduce the effects on the performance of the Models when dealing with highly homogenous or otherwise biased data sets.

>Also, there has not been enough time to actually spend time on further anylizing the results yet or test trained models on other Test Set datas, so the information about how transferable different Models actually are is still missing, even tho, assumptions and conclusions could be drawn from the Models (and it looks like, they are not too transferable atleast in perspective, that they actually could have been used for speaker verification with other data sets)

>Learning wise I have achieved to learn more about data processing in general and generating (Siamese) Deep Neural Network Models using Python. Also I have learned some tricks on data sampling and other more general data-science related methods, and have become more familiar with the tools used in this field.

# Methods

There are three major components of a method you must describe:

* _Programming_: which libraries is used for which purpose.

> Utility libraries like zipfile, os, random etc. were used for data fetching from the source and for some of the pre-processing.

> Most of the libraries used in this project were basic data-science libraries, like NumPy, Pandas and Sklearn. These libraries were used for processing data and to help utilizing the Neural Network Model.

> Tensorflow.Keras and other tensorflow libraries were used for creating, training and testing the Neural Network Models.

* _Data_: which dataset has been used to validate your hypothesis.

>AudioMNIST data set was used for creating four different Train-and Test Sets simulating different scenarios in speaker verification.

* _Algorithm_: which machine learning algorithms is used for the task, describe all the algorithm briefly as you could, a flow diagram or pseudocode is the best option if you have multiple-step in a algorithm.

>The Machine Learning Algorithm for training the Siamese Neural Network model was following:

>> get for DataFrame consisting of trainable pairs of utterances from all Train Set samples

>> for sample pairs in DataFrame:

>>>>take a sample pair

>>>if pair from same speaker:

>>>>set target variable y: y = 1, else set y = 0

>>>>get MFCC-features from pairs

>>>set and normalize weights based on occurances of same vs different speakers among all of the formed pairs 

>>>train siamese model(inputs=[mfcc_feats1, mfcc_feats2], target = y, weights={w_different, w_same})

>In the algorithm, the input samples (MFCC-features) are passed into a Siamese Neural Network Model, which consists of Deep Concolutional Neural Network Model, which is used for both samples (for weigth sharing). 

>Then the results are mapped into two feature maps and embedded together. Then eouclidean distance is calculated for both feature maps, in order to determine, the similarity among the samples.

> Finally a single sigmoided number is outputted from the output layer indicating if the speakers were same(close to 1) or different (close to 0).

## Used libraries

Just provide a list of library you used here, and write a (very short) description of their function (or why you use them). For example:

* _numpy_: for data processing and utility functions.
* _pandas_: for data processing and saving Train-and Test Sets.
* _sklearn.metrics import det_curve, DetCurveDisplay_: for evaluation visualization
* _sklearn.model_selection import train_test_split_: for generating training and test sets without looping.
* _tensorflow_: for tensorflow.keras usage.
* _tensorflow import keras_: for generating, training and testing neural Network Models.
* _tensorflow.keras.callbacks import ModelCheckpoint_: for generating checkpoints during training of Neural Network.
* _tensorflow.keras.layers import Input_: for setting inputs to be passed into Siamese Neural Network Model.
* _tensorflow.keras.layers import Subtract, Lambda_: for embedding the feature maps and calculating the euclidean distance between them.
* _tensorflow.keras.models import Model_: for Siamese Neural Network Model creation.
* _tensorflow.keras.backend_: for utilixing keras backend functions on embedding and euclidean distance calculations.
* _librosa_: for audio signal processing.
* _librosa import display_: for displaying audio data.
* _os, glob, pathlib, zipfile_: for data pre-processing.
* _urllib.request import urlretrieve_: for fetching AudioMNIST data.
* _matplotlib_: for data visualization

Run the following line to install nighty build of sklearn before importing libraries (uninstall of old sklearn required):
"""

!pip uninstall scikit-learn -y
!pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn

# Import ds libraries.
import numpy as np
import pandas as pd
from sklearn.metrics import det_curve, DetCurveDisplay
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Input, Subtract, Lambda
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K

# Import audio libraries.
import librosa
from librosa import display

# Import utilities.
import os
import glob
from urllib.request import urlretrieve
from pathlib import Path
import random
from zipfile import ZipFile
import ast

# Import plotting
import matplotlib.pyplot as plt

"""## Datasets

What are the features in the given dataset?

>The data set consists of wavefiles, containing utterances from spoken digits between numbers 0-9. From the audio file, many features could have been draw, from different spectrogram presentations, to differently splitted signal samples and to MFCC-features and chunks of them and also filter-banks and their representative variations.

How do you split the dataset for training and evaluation?

>The data set was split into four different Train and Test Sets as following:

>Basic scenario: Data is split into Train Set containing 20000 samples from 40 speakers and Test Set containing 10000 samples from 20 other speakers.

>Text dependent scenario: Data is split into Train Set containing 1500 samples
from single spoken utterance and Test set containing an other 1500 samples from the same spoken digit from different utterances.

>Text limited scenario: Data is split into Train Set containing 15000 samples consisting of 50% of the utterances per digit per speaker and Test Set containing 15000 samples consisting the other half of the data.

>Text independent scenario: Data is split into Train Set containing of 15000 samples consisting of all utterances from five randomly chosen digits and Test Set containing 15000 samples consisting of all utterances from the remaining 5 digits.

Which features (input variables) are you going to use?

>The MFCC-features were taken from the utterances for each set and used as the inputs for the Deep Neural Network.

Which target variables are needed?

>The target variable needed for training, is simply the binary-classification if the two samples about to be prcessed via the Model are from the same or not the same speaker.

>The target variables for testing for all scenarios except the *basic scenario* were also based on the binary classifications if the Test Set samples were from the same speaker or not. For the *basic scenario*, *target score* and *non-target score* sets with sizes of 10 and 50 samples, were separated from the Test Set and used for predicting and evaluating the Model.

What is the possible connection between these variables?

> The connection between the variables is in the class, which they represent.

First we have to define couple of basic functions, in order to get the splitting done correctly
"""

# Directory scan function.
def dirscan(dir):
    subfolders = [f.path for f in os.scandir(dir) if f.is_dir()]
    for dir in list(subfolders):
        subfolders.extend(dirscan(dir))
    return subfolders

# Define pre-emphasis function.
def preemphasis(signal, coeff=0.95):

    # Emphassized signal
    signal_out = np.append(signal[0], signal[1:] - coeff * signal[:-1])

    # Convert to numpy-array.
    signal_out = np.array(signal_out)

    # Return pre-emphasized signal
    return signal_out


# Get MFCCs.
def mfccs(file_path: str,
          preemphasis_coeff: float = 0.95,
          n_fft: int = 512,
          winlen: float = 0.025,
          winstep: float = 0.01,
          n_filter_banks: int = 40,
          n_mfccs: int = 13,
          winfunc: callable = np.hamming,
          sampling_rate=8000,
          verbose: bool = False) -> np.ndarray:

    # Reassemble the sampling rate if needed.
    y, sr = librosa.load(file_path, sr=None)
    if sr != sampling_rate:
        y = librosa.resample(y, orig_sr=sr, target_sr=sampling_rate)

    # Pre-emphasize the signal.
    y = preemphasis(y, preemphasis_coeff)

    # Define window length and hop-length.
    window_length = int(winlen * sampling_rate)
    hop_length = int(winstep * sampling_rate)

    # Get magnitude spectrogram.
    spectrogram = np.abs(librosa.stft(y,
                                      n_fft=n_fft,
                                      hop_length=hop_length,
                                      win_length=window_length,
                                      center=False))
    # Get mel-spectrogram.
    mel_spectrogram = librosa.feature.melspectrogram(S=spectrogram)

    # Get MFCC features from the mel-spectrogram..
    mfccs = librosa.feature.mfcc(S=np.log(mel_spectrogram), n_mfcc=n_mfccs)

    # Normalize MFCCs by mean and standard-deviation.
    mfccs_norm = mfccs - np.mean(mfccs, axis=1, keepdims=True)
    mfccs_norm /= np.std(mfccs, ddof=0, axis=1, keepdims=True)

    # Visualization for debugging
    if verbose:
        plt.figure(figsize=(10, 12))
        librosa.display.waveplot(y, sr=sr, ax=plt.subplot(3, 2, 1))
        plt.gca().set_title('Wave plot')
        librosa.display.specshow(librosa.amplitude_to_db(spectrogram),
                                 sr=sr,
                                 hop_length=hop_length,
                                 fmax=sampling_rate / 2,
                                 y_axis='linear',
                                 x_axis='time',
                                 ax=plt.subplot(3, 2, 2))
        plt.gca().set_title('Spectrogram')
        librosa.display.specshow(librosa.amplitude_to_db(mel_spectrogram),
                                 sr=sr,
                                 hop_length=hop_length,
                                 fmax=sampling_rate / 2,
                                 y_axis='mel',
                                 x_axis='time',
                                 ax=plt.subplot(3, 2, 3))
        plt.gca().set_title('Mels-Spectrogram')
        librosa.display.specshow(librosa.amplitude_to_db(mfccs),
                                 sr=sr,
                                 fmax=sampling_rate / 2,
                                 x_axis='time',
                                 hop_length=hop_length,
                                 ax=plt.subplot(3, 2, 4))
        plt.gca().set_title('MFCCs')
        ax = plt.subplot(3, 2, 5)
        ax.plot(mfccs)
        ax.set_title('Individual MFCC features')
        ax = plt.subplot(3, 2, 6)
        ax.plot(mfccs_norm)
        ax.set_title('Normalized MFCCs')
        plt.tight_layout()
    return mfccs_norm.T

# Function for unifying MFCC frame-length.
def unify_mfcc(standard_length, mfcc):

    # If maximum length > frame length, do padding to end.
    if (standard_length > mfcc.shape[0]):
        pad_width = standard_length - mfcc.shape[0]
        mfcc = np.pad(mfcc, ((0,pad_width), (0,0)), mode='constant', constant_values=(0,0))

    # Else do cutoff from end.
    else:
        mfcc = mfcc[:standard_length, :]

    # Return unified MFCC.
    return mfcc

"""Download the data set and get utterances:"""

# Download data set and extract files.
url = r"https://github.com/soerenab/AudioMNIST/archive/master.zip"
filename = "/tmp/data.zip"
if not os.path.exists(filename):
    print('Downloading AudioMNIST dataset ...')
    urlretrieve(url, filename=filename)
path = '/tmp/AudioMNIST-master/data'
if not os.path.exists(path):
    print("Extracting files from AudioMNIST...")
    with ZipFile(filename, mode='r') as f:
        f.extractall(path="/tmp")

# Get utterances.
path = "/tmp/AudioMNIST-master/data"
subfolders = dirscan(path)

utterances = []

# Iterate through subfolders and add all wavfiles under same variable.
for folder in subfolders:
    path = Path(folder).absolute()
    for wavefile in glob.glob(os.path.join(path, '*.wav')):
        utterances.append(wavefile)

np.random.shuffle(utterances)

metadata = [os.path.basename(i).split('_')[:-1] for i in utterances]
digits = np.array([i[0] for i in metadata])
speakers = np.array([i[1] for i in metadata])

print(utterances[:5])
print(metadata[:5])
print(digits[:5])
print(speakers[:5])

"""You can mount your Google Drive, if you want to save following csv-files etc to you Drive:"""

# Mount Google Drive.
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

"""Create set of unique speakers for later use:"""

# Unique speakers.
speakers_set = np.unique(speakers)

"""Following code block is the most time-requiring among data processing as it loads the whole mfcc-features set and also creates the "main" DataFrame from the utterances and it's metadata (depending on machine, usually took ~20min with GPU-acceleration and using NVidia GTX1050 on local machine and ~1h with Colab). After run once, this block can be skipped by loading the saved Pickle-file. (if needed, you can also load it to you Google Drive in order to not be distracted by the runtime-disconnection issues)"""

# Split to Train Sets and Test Sets based on following conditions:

# DataFrame containing all utterances, speakers and digits.
all_data = pd.DataFrame()

# Counter variable.
counter = 0

# Define standard length for frames in MFCC.
standard_length = 60

# For utterance.
for file in utterances:
    # Do processing into utterance, digit and speaker.
    splitted = os.path.basename(file).split('_')[:-1]
    digit = int(splitted[0])
    speaker = int(splitted[1])  # Get rid of zeros for easier indexing.
    feats = mfccs(file) # Get mfcc.
    #Reshape to rough estimate in order to maintain unified shape between samples.
    feats = unify_mfcc(standard_length, feats)
    # Just in case save also to its own 3D array.
    all_data = all_data.append(pd.DataFrame({"utt": ["{}".format(file)], "dgt": [digit], "spk": [speaker], "mfcc": [feats]}, index=[counter]))
    counter += 1

# Save to Pickle for later use to runtime.
all_data.to_pickle("/tmp/meta_mfcc.pkl")

"""Run the following block if you want to save Pickle to your Drive:"""

# Save Pickle to Google Drive. Path can be changed if wanted to specify different folder.
path = "/content/drive/MyDrive/meta_mfcc.pkl"
all_data.to_pickle("{}".format(path))

"""Run the following block if you want to load the Pickle from runtime:"""

# Load Pickle from runtime (if still there)
all_data = pd.read_pickle("/tmp/meta_mfcc.pkl")
print(all_data.head())

"""Run the following if you want to load Pickle from the Google Drive:"""

# Load Pickle from Google Drive.
path = "/content/drive/MyDrive/meta_mfcc.pkl"
all_data = pd.read_pickle("{}".format(path))
print(all_data.head())

"""Split data into 4 different Train Sets and Test Sets:"""

"""Train Set and Test Set 1: Contains "basic" scenario, where all data is splitted into Train Set and Test Set, later on the train set 
is used to train the network by picking random samples from the Train Set and is then tested using "target scores" containing 10 randomly 
chosen utterances from the same speaker and 50 randomly chosen utterances from an other speaker on the Test Set. Please notice, that the
target scores and non-target scores are generated from the Test Set in the evaluation phase and only the main Set for testing is generated 
here."""

# Train Set 1.
train_set_basic = pd.DataFrame()

# Test Set 1.
test_set_basic = pd.DataFrame()

# New speakers set (formatted).
speakers_set_formatted = []

# Process digits away from unique speakers set.
for speaker in speakers_set:
    speaker = int(str(speaker))
    speakers_set_formatted.append(speaker)

# Take 40 speakers at random from the  spekers_set as Training Set.
train_spk = []
sample_speakers = random.sample(list(speakers_set_formatted), len(speakers_set_formatted) - 20)
for speaker in sample_speakers:
    train_spk.append(speaker)

# Take remaining 20 speakers (not in Train Set) as Test Set.
test_spk = []
for speaker in speakers_set_formatted:
    if np.isin(speaker, train_spk) == False:
        test_spk.append(speaker)

# For each speaker in Train Set speakers.
for i in range(len(train_spk)):
    wanted = train_spk[i]
    speaker = all_data["spk"] == wanted
    train_set_basic = train_set_basic.append(all_data[speaker])

# For each speaker in Test Set speakers.
for i in range(len(test_spk)):
    wanted = test_spk[i]
    speaker = all_data["spk"] == wanted
    test_set_basic = test_set_basic.append(all_data[speaker])

print("Train Set Basic head:\n {}\nShape: {}".format(train_set_basic.head(), train_set_basic.shape))
print("Test Set Basic head:\n {}\nShape: {}".format(test_set_basic.head(), test_set_basic.shape))


plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.hist(train_set_basic["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Train Set Basic speakers')
plt.subplot(1, 2, 2)
plt.hist(test_set_basic["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Test Set Basic speakers')
plt.tight_layout()
plt.show()

"""Train Set and Test Set 2: Contains context Dependent data (only same digit)
   Data is splitted so, that the Train Set and Test set both contain only same digits from all speakers
   for example digit 1 from all speakers. Train Set contains 50% of the digits and the Test set other 50%.
   Note that the 50% mark is not necessarily the splitting distribution between the digits spoken from same speaker 
   (set may contain 100% of digits y from speaker x and 0% from an other in this scenario)."""

# Variable to define which digit is used for set making.
used_digit = 4

# DF for all utterances from chosen digit.
all_utts_from_digit = pd.DataFrame()

# Get all utts based on digit.
wanted = all_data["dgt"] == used_digit
all_utts_from_digit = all_data[wanted]

# Train Set 2.
train_set_dependent = pd.DataFrame()

# Test Set 2.
test_set_dependent = pd.DataFrame()

# Split into half, other half as Train Set and other as Test set.
train_set_dependent, test_set_dependent = train_test_split(all_utts_from_digit, test_size=0.5)

print("Train Set context Dependent head:\n {}\nShape: {}".format(train_set_dependent.head(), train_set_dependent.shape))
print("Test Set context Dependent head:\n {}\nShape: {}".format(test_set_dependent.head(), test_set_dependent.shape))

plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.hist(train_set_dependent["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Train Set Dependent speakers')
plt.subplot(1, 2, 2)
plt.hist(test_set_dependent["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Test Set Dependent speakers')
plt.tight_layout()
plt.show()

"""Train Set and Test Set 3: Contains context Limited data, where the whole data set is included.
   With this setup, 50% of the digits and utterances per speaker are distributed into Train Set and the other 50% into Test Set."""

# Train Set 3.
train_set_limited = pd.DataFrame()

# Test Set 3.
test_set_limited = pd.DataFrame()

# For number of digits.
for j in range(0, 10):
    # For number of speakers.
    for i in range(1, 61):

        # Get speaker and digit according to loop iterators.
        speakers = all_data["spk"] == i
        digits = all_data["dgt"] == j

        # Gather data cells from DF, where digit and speaker are
        # corresponding to the loop iterators.
        data = all_data[speakers & digits]

        # Divider for the halves to be added to sets.
        train_temp, test_temp = train_test_split(data, test_size=0.5)

        # Append halves to actual sets.
        train_set_limited = train_set_limited.append(train_temp)
        test_set_limited = test_set_limited.append(test_temp)

print("Train Set Text Limited head:\n {}\nShape: {}".format(train_set_limited.head(), train_set_limited.shape))
print("Test Set Text Limited head:\n {}\nShape: {}".format(test_set_limited.head(), test_set_limited.shape))

plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.hist(train_set_limited["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Train Set Limited speakers')
plt.subplot(1, 2, 2)
plt.hist(test_set_limited["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Test Set Limited speakers')
plt.tight_layout()
plt.show()

"""Train Set and Test Set 4: Contains context Independent data, where Train Set contains all utterances of 5 digits from the Data Set
   and the Test Set contains an other 5 digits from the Data Set (which are not in the Train Set)"""

# Generate list of digits from 0-9.
generated_digits = []
for i in range(10):
    generated_digits.append(i)

# Take digits for Train Set from generated list using random sampling.
train_digits = random.sample(list(generated_digits), len(generated_digits) // 2)

# Populate Test Set digits from the ones which were not included into Train Set digits.
test_digits = []
for digit in generated_digits:
    if np.isin(digit, train_digits) == False:
        test_digits.append(digit)

print("Train Set digits: {}".format(train_digits))
print("Test Set digits: {}".format(test_digits))

# Train Set 4.
train_set_independent = pd.DataFrame()

# Test Set 4.
test_set_independent = pd.DataFrame()

for i in range(5):
    wanted = train_digits[i]
    wanted_digit = all_data["dgt"] == wanted
    train_set_independent = train_set_independent.append(all_data[wanted_digit])
for j in range(5):
    wanted = test_digits[j]
    wanted_digit = all_data["dgt"] == wanted
    test_set_independent = test_set_independent.append(all_data[wanted_digit])

print("Train Set context Independent head:\n {}\nShape: {}".format(train_set_independent.head(), train_set_independent.shape))
print("Test Set context Independent head:\n {}\nShape: {}".format(test_set_independent.head(), test_set_independent.shape))

plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.hist(train_set_independent["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Train Set Independent speakers')
plt.subplot(1, 2, 2)
plt.hist(test_set_independent["spk"])
plt.xlabel("Speaker ID")
plt.ylabel("Samples")
plt.title('Test Set Independent speakers')
plt.tight_layout()
plt.show()

"""You can test MFCC-fetching with this premade function (fetches MFCCs of one utterance per Set and visualized the process):"""

# Testing MFCC-fetch once for each set to see they work.
def test_mfcc_fetch():
    # Set verbose to True, if you want to plot the results.
    
    # Context Basic Sets.
    _ = mfccs(train_set_basic["utt"].iloc[0], verbose=True)
    _ = mfccs(test_set_basic["utt"].iloc[0], verbose=True)

    # Context Dependent Sets.
    _ = mfccs(train_set_dependent['utt'].iloc[0], verbose=True)
    _ = mfccs(test_set_dependent['utt'].iloc[0], verbose=True)

    # Context Limited Sets.
    _ = mfccs(train_set_limited['utt'].iloc[0], verbose=True)
    _ = mfccs(test_set_limited['utt'].iloc[0], verbose=True)

    # Context Independent Sets.
    _ = mfccs(train_set_independent['utt'].iloc[0], verbose=True)
    _ = mfccs(test_set_independent['utt'].iloc[0], verbose=True)

test_mfcc_fetch()

"""Write down some critical analysis of the data, for example, the data is imbalance, some biases might be presented in the classifier.

> Data is pretty decently distributed overall, but the Train Sets and Test Sets for some of the scenarios are definitely biased by the fact that for example the context dependent set is both trained and tested on the utterances from the same digit, which obviously should affect the classification results (as it is more likely that even the different speakers sound more similar saying same digits) and also other sets are affected by similar type of scenarios. 

> Also some bias can be randomly generated depending on the batch, as there might be a lack of samples from one speaker in a Set and thus it affects the training results.

> However this is purposeful for testing the scenarios and later on  as it would be sufficient to see how the Models trained in a biased environment would do with classification tasks on other Test Sets, which is why the data splitting in this manner is beneficial.

## Model description

Which algorithm you used for solving the task?

> Siamese Deep Neural Network was trained using custom algorithm provided earlier in this notebook on section **Methods**

Write down technical description of the algorithm (maths and steps).

* How to initialize the model?
>The model is initialized in two steps:

>>> 1: The Base Model is created, consisting of Deep Convolutional Neural Network, with 2 Convolutional layer, followed by 2 Maximum Pooling layers. Then 2 more Convolutional layers are applied and finally 2 Dense layers, with ReLU activation are applied. Batch normalization was also applied to all Convolutional layers.

>>>2: The Siamese Deep Neural Network Model is initilized with 2 input layers, which are connected to the Base-line Model. Then the results from the base-line Model are mapped into feature maps for each input in the Siamese Model. After that, the feature map embedding is applied and euclidean distance between them is calculated. Finally a Dense output layer with sigmoid activation is applied to provide prediction of the class.

>>> Adam-optimizer with learning rate = 0.001, with loss = Binary-crossentropy and with metrics = binary-accuracy was used for the Model.

* Which parameters are important and relevant for our dataset?

> The most important parameters are the similarity scores calculated from the two input features (MFCC-features).

* How we fit the model?

> All Models are fitted in a training function, where the corresponding Train Set is devided into pairs, from which the actual similarity (target score) is calculated and then the mfcc-features are separated. We feed all the pairs to the network, with their corresponding target scores, and then the Model is trained for chosen number of epochs in quite large batches based on the Train Set size.

> Also a funtionality to fit the Model with less biased Data is afforded in this project and used for training four other Models for reference.

> As the some of the data sets are biased in a way, that the propability of the same speaker is much lower than the probability of speakers being different, the weights are adjusted based on the calculated number of occurances of cases where the speakers in the pairwise splitted set are actually the same.

* How do we perform prediction and evaluation with the model?

> Model is predicted and evaluated in very similar manner as the fitting of the model is done, except we are using the pre-splitted Test Sets instead of Train Sets, the weights are not adjusted, and the  prediction is done without Model knowing the right value of the class.

> For the basic scenario, the data is split into target scores and non-target scores for this purpose, and values from those are used, instead of the pairs.

> For all of the Models predictions and evaluations were also drawn from multiple other Data Sets beyound the dedicated Test Set and the Models can be thus compared performance wise against each other to see if some of them were actually more transferable than others.

Then provide the implementation below.

**NOTE**: _To show your understanding, it is the best to provide a flow diagram or a figure of model component, you could inlcude an image using the following code_

`<img src="..." alt="..." width="700"/>`

<img src="https://drive.google.com/uc?export=view&id=1ZOEuy1Q46IaEa_Ta3C1UWm0QmIoe_YCV" alt="Siamese Neural Network Model" width="500"/>

Build Base Model:
"""

# Build Base Model for Siamese Neural Network.
def build_model():
    # Create base model.
    base_model = keras.Sequential([

        # 2 times convolutions and max pooling.
        keras.layers.Conv2D(name="1_C", filters=32, kernel_size=(5,5), padding="same", input_shape=(60, 13, 1), activation ="relu"),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(name="M_1", pool_size=2, strides=2),

        keras.layers.Conv2D(name="2_C", filters=40, kernel_size=(5,5), padding="same", activation="relu"),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(name="M_2", pool_size=2, strides=2),

        # 2 times convolutions without max pooling.
        keras.layers.Conv2D(name="3_C", filters=40, kernel_size=(1,1), padding="same", activation="relu"),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(name="4_C", filters=12, kernel_size=(3,3), padding="same"),
        keras.layers.BatchNormalization(),

        keras.layers.Flatten(),
        keras.layers.Dense(432, activation="relu"),
        keras.layers.Dense(60, activation="relu"),
    ])

    # Initialize optimizer.
    optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Adam  optimizer.

    base_model.compile(optimizer=optimizer, loss=keras.losses.binary_crossentropy, metrics=["accuracy"],
                       sample_weight_mode=None)

    base_model.build((60, 13, 1))

    print("Base Model initialized successfully")

    return base_model

"""Build Siamese Model:"""

# Build a Siamese Neural Network for Automatic Speaker Verification (AVS).
def build_siamese(base_model, input_shape):

    # Set inputs to shape.
    sample1 = tf.keras.Input(shape=input_shape)
    sample2 = tf.keras.Input(shape=input_shape)

    # Get feature maps by samples from NNW model.
    encoded_sample1 = base_model(sample1)
    encoded_sample2 = base_model(sample2)

    # Calculate euclidean distance.
    euclidean_dist = Subtract()([encoded_sample1, encoded_sample2])
    euclidean_dist = Lambda(lambda x: K.sqrt(K.mean(K.square(x), axis=-1, keepdims=True)))(euclidean_dist)

    # Output single sigmoidian number defining if two sample utterances were from same speaker (~1 = same, ~0 = different).
    prediction = keras.layers.Dense(1, activation="sigmoid")(euclidean_dist)

    siamese_model = Model(inputs=[sample1, sample2], outputs=prediction)

    # Initialize optimizer.
    optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Adam optimizer.

    siamese_model.compile(optimizer = optimizer, loss = keras.losses.binary_crossentropy, metrics = ["accuracy"], sample_weight_mode=None)

    siamese_model.build((120, 26, 1))

    print("Siamese model build succesfully")

    return siamese_model

"""Function to fit the Model:"""

# Trains the given Siamese Model with given Train Set.
def train_siamese(train_samples: pd.DataFrame, siamese_model: Model = None, verbose: bool = False):

    # Get indices from Sample Set.
    indices = np.arange(len(train_samples))
    np.random.shuffle(indices)
    indices_clone = np.copy(indices)
    np.random.shuffle(indices_clone)

    # Generate Sample Set as pairwise DataFrame based on indices.
    pairwise_map = pd.DataFrame({"sample1": indices, "sample2": indices_clone})

    # Take speakers from Sample Set.
    speaker_sample1 = train_samples["spk"].iloc[pairwise_map["sample1"].values].values
    speaker_sample2 = train_samples["spk"].iloc[pairwise_map["sample2"].values].values

    # Take MFCCs for Sample Set.
    mfccs_1 = train_samples["mfcc"].iloc[speaker_sample1].values
    mfccs_2 = train_samples["mfcc"].iloc[speaker_sample2].values

    # Generate correct input format for NNW.
    mfccs_inputs_1 = np.zeros((len(train_samples), 60, 13))
    mfccs_inputs_2 = np.zeros((len(train_samples), 60, 13))
    for i in range(len(train_samples)):
        mfccs_inputs_1[i,:] = mfccs_1[i]
        mfccs_inputs_2[i,:] = mfccs_2[i]

    # Sanity checks.
    print(mfccs_inputs_1.shape)
    print(mfccs_inputs_2.shape)

    # Reshape.
    mfccs_inputs_1 = mfccs_inputs_1.reshape((-1, 60, 13, 1))
    mfccs_inputs_2 = mfccs_inputs_2.reshape((-1, 60, 13, 1))

    # Check if same speaker.
    y = np.equal(speaker_sample1, speaker_sample2)

    # Get sum of how many speakers were same and adjust weigths.
    weight_zero = int(np.sum(y))/int(len(train_samples)-np.sum(y)) # For not same speaker 0s.
    weight_one = 1 # For same speaker 1s.

    # Define batch size.
    batch_size = len(mfccs_inputs_1)//40

    # If Model is passed, dont build new.
    if siamese_model == None:
        # Build Base Model.
        base_model = build_model()

        # Build Siamese NNW.
        siamese_model = build_siamese(base_model,(60,13,1))

    # If verbose is true, print Model summaries.
    if verbose == True:
        base_model.summary()
        siamese_model.summary()

    # Feed MFCC features from samples to Siamese NNW and train it based on
    # if utterances were from same or not same person.

    # Train Model.
    siamese_model.fit(x=[mfccs_inputs_1, mfccs_inputs_2], y=y,
                      validation_split=0.2, batch_size=batch_size,
                      epochs=100, shuffle=True, verbose=2, class_weight={0: weight_zero, 1: weight_one})

    # Return trained Model.
    return siamese_model

# Generate Siamese Models and train them for each set (to be evaluated later). 
# To see Model summaries, set verbose=True (not set true for first Model to see the summary).
siamese_model_basic = train_siamese(train_set_basic, verbose=True)

siamese_model_dependent = train_siamese(train_set_dependent, verbose=False)

siamese_model_limited = train_siamese(train_set_limited, verbose=False)

siamese_model_independent = train_siamese(train_set_independent, verbose=False)

"""If you want to save trained Models to runtime or Drive, run following (after mounting if Drive), change booleans:"""

save_temp = False
save_drive = True

if save_temp == True:
   # Save trained models.
    siamese_model_basic.save("basic_model.h5")
    siamese_model_dependent.save("dependent_model.h5")
    siamese_model_limited.save("limited_model.h5")
    siamese_model_independent.save("independent_model.h5")

if save_drive == True:
  # Save trained models to Drive.
  path = "/content/drive/MyDrive/basic_model.h5"
  siamese_model_basic.save(path)

  path = "/content/drive/MyDrive/dependent_model.h5"
  siamese_model_dependent.save(path)

  path = "/content/drive/MyDrive/limited_model.h5"
  siamese_model_limited.save(path)

  path = "/content/drive/MyDrive/independent_model.h5"
  siamese_model_independent.save(path)

"""To load Models from the runtime or Drive run this block (change booleans):"""

load_temp = False
load_drive = True

if load_temp == True:
  # Load trained models for re-use from runtime.
  siamese_model_basic = tf.keras.models.load_model("basic_model.h5")
  siamese_model_dependent = tf.keras.models.load_model("dependent_model.h5")
  siamese_model_limited = tf.keras.models.load_model("limited_model.h5")
  siamese_model_independent = tf.keras.models.load_model("independent_model.h5")

if load_drive == True:
  # Load trained models for re-use from Drive.
  path = "/content/drive/MyDrive/basic_model.h5"
  siamese_model_basic = tf.keras.models.load_model(path)

  path = "/content/drive/MyDrive/dependent_model.h5"
  siamese_model_dependent = tf.keras.models.load_model(path)

  path = "/content/drive/MyDrive/limited_model.h5"
  siamese_model_limited = tf.keras.models.load_model(path)

  path = "/content/drive/MyDrive/independent_model.h5"
  siamese_model_independent = tf.keras.models.load_model(path)

"""Function for fitting Model with less biased data (samples more evenly spread between same / not same speakers):"""

def train_siamese_equal(train_samples: pd.DataFrame, siamese_model: Model = None, verbose: bool = False):
  
    # Get indices from Sample Set.
    indices = np.arange(len(train_samples))
    np.random.shuffle(indices)
    indices_clone = np.copy(indices)
    np.random.shuffle(indices_clone)

    # Generate Sample Set as pairwise DataFrame based on indices.
    pairwise_map = pd.DataFrame({"sample1": indices, "sample2": indices_clone})

    # Take speakers from Sample Set.
    speaker_sample1 = train_samples["spk"].iloc[pairwise_map["sample1"].values].values
    speaker_sample2 = []

    # Take MFCCs for Sample Set.
    mfccs_1 = train_samples["mfcc"].iloc[speaker_sample1].values

    # Generate Target-scores MFCC scores for half of the utterances from same speaker for each speaker in first MFCC set.
    mfccs_2_target = []
    for i in range(len(speaker_sample1)//2):
        speaker = speaker_sample1[i]
        speaker_sample2.append(speaker)
        mfccs_speaker = train_samples.loc[train_samples["spk"] == speaker, "mfcc"]
        mfccs_target = random.sample(list(mfccs_speaker), 1)
        mfccs_2_target.append(mfccs_target)
    
    # Generate Non-target scores for the other half.
    mfccs_2_non_target = []
    for i in range(len(speaker_sample1)//2):
        speaker = speaker_sample1[i]
        not_speaker = train_samples.loc[train_samples["spk"] != speaker, "spk"].iloc[0]
        speaker_sample2.append(not_speaker)
        mfccs_not_speaker = train_samples.loc[train_samples["spk"] == not_speaker, "mfcc"]
        mfccs_non_target = random.sample(list(mfccs_not_speaker), 1)
        mfccs_2_non_target.append(mfccs_non_target)
    
    # Convert to numpy.
    speaker_sample2 = np.array(speaker_sample2)

    # Combine lists.
    mfccs_2 = mfccs_2_target + mfccs_2_non_target
    mfccs_2 = np.array(mfccs_2)
    
    # Generate correct input format for NNW.
    mfccs_inputs_1 = np.zeros((len(train_samples), 60, 13))
    mfccs_inputs_2 = np.zeros((len(train_samples), 60, 13))
    for i in range(len(train_samples)):
        mfccs_inputs_1[i,:] = mfccs_1[i]
        mfccs_inputs_2[i,:] = mfccs_2[i]

    # Sanity checks.
    print(mfccs_inputs_1.shape)
    print(mfccs_inputs_2.shape)

    # Reshape.
    mfccs_inputs_1 = mfccs_inputs_1.reshape((-1, 60, 13, 1))
    mfccs_inputs_2 = mfccs_inputs_2.reshape((-1, 60, 13, 1))

    # Check if same speaker.
    y = np.equal(speaker_sample1, speaker_sample2)

    # Get sum of how many speakers were same and adjust weigths.
    weight_zero = int(np.sum(y))/int(len(train_samples)-np.sum(y)) # For not same speaker 0s.
    weight_one = 1 # For same speaker 1s.

    # Define batch size.
    batch_size = len(mfccs_inputs_1)//40

    # If Model is passed, dont build new.
    if siamese_model == None:
        # Build Base Model.
        base_model = build_model()

        # Build Siamese NNW.
        siamese_model = build_siamese(base_model,(60,13,1))

    # If verbose is true, print Model summaries.
    if verbose == True:
        base_model.summary()
        siamese_model.summary()

    # Feed MFCC features from samples to Siamese NNW and train it based on
    # if utterances were from same or not same person.

    # Train Model.
    siamese_model.fit(x=[mfccs_inputs_1, mfccs_inputs_2], y=y,
                      validation_split=0.2, batch_size=batch_size,
                      epochs=50, shuffle=True, verbose=2, class_weight={0: weight_zero, 1: weight_one})

    # Return trained Model.
    return siamese_model

# Generate Siamese Models and train them for each set (to be evaluated later). 
# To see Model summaries, set verbose=True (not set true for first Model to see the summary).
siamese_model_basic_equal = train_siamese_equal(train_set_basic, verbose=True)

siamese_model_dependent_equal = train_siamese_equal(train_set_dependent, verbose=False)

siamese_model_limited_equal = train_siamese_equal(train_set_limited, verbose=False)

siamese_model_independent_equal = train_siamese_equal(train_set_independent, verbose=False)

save_temp = False
save_drive = True

if save_temp == True:
   # Save trained models.
    siamese_model_basic_equal.save("basic_model_equal.h5")
    siamese_model_dependent_equal.save("dependent_model_equal.h5")
    siamese_model_limited_equal.save("limited_model_equal.h5")
    siamese_model_independent_equal.save("independent_model_equal.h5")

if save_drive == True:
  # Save trained models to Drive.
  path = "/content/drive/MyDrive/basic_model_equal.h5"
  siamese_model_basic_equal.save(path)

  path = "/content/drive/MyDrive/dependent_model_equal.h5"
  siamese_model_dependent_equal.save(path)

  path = "/content/drive/MyDrive/limited_model_equal.h5"
  siamese_model_limited_equal.save(path)

  path = "/content/drive/MyDrive/independent_model_equal.h5"
  siamese_model_independent_equal.save(path)

load_temp = False
load_drive = True

if load_temp == True:
  # Load trained models for re-use from runtime.
  siamese_model_basic_equal = tf.keras.models.load_model("basic_model_equal.h5")
  siamese_model_dependent_equal = tf.keras.models.load_model("dependent_model_equal.h5")
  siamese_model_limited_equal = tf.keras.models.load_model("limited_model_equal.h5")
  siamese_model_independent_equal = tf.keras.models.load_model("independent_model_equal.h5")

if load_drive == True:
  # Load trained models for re-use from Drive.
  path = "/content/drive/MyDrive/basic_model_equal.h5"
  siamese_model_basic_equal = tf.keras.models.load_model(path)

  path = "/content/drive/MyDrive/dependent_model_equal.h5"
  siamese_model_dependent_equal = tf.keras.models.load_model(path)

  path = "/content/drive/MyDrive/limited_model_equal.h5"
  siamese_model_limited_equal = tf.keras.models.load_model(path)

  path = "/content/drive/MyDrive/independent_model_equal.h5"
  siamese_model_independent_equal = tf.keras.models.load_model(path)

"""If any issues arise when fitting or predicting, explain why?

>There is some issues with context dependent and limited sets, as their loss-seems to not reduce, which indicates, that the Model is not learning at all. This might be partially, because of the really biased data on the Train Sets as tehy are formed using only same digit or portion that has huge variance.

# Experiments and results

Describe your experiments setup

* which metrics are used for evaluation?

> Right now the binary-crossentropy is used as loss metric for the model and binary-accuracy is used for the accuracy metric.

> The prediction for speaker verification is based on the similarity scores between given samples and the actual target value.

* what are the baseline methods?

> Not one specific base-line method was found or applied as all the previous works found were done with too different types of setups to actually prove worthy for trying.

> Trying to accomplish in generating my own base-line method also did not succeed very well...

* How do you run the experiments?

> The Models are trained individually on dedicated Train Sets and then predictions are drawn and they are evaluated indivudually on the dedicated Test Sets. The dedicated Sets are used for training Models in speficif scenarios, where the training data and testing data are formed in a way, that certain types of result could be expected.

> The Models are trained for 100 epochs and with relatively large batch size. Lots of different variations on epochs and batch sizes were tested, but they yielded very varying result, which were less promising.

> Later on the Models are used to predict on new unseen data from totally different scenario and evaluated on it, in order to evaluate the transferability of the set.

>Also noteable is, that the Models can be saved, in order to continue training and evaluation later on.

* And finally what is the results and your analysis, also some discussion.

> The result of the anylysis is, that the Models seemed to lear well, but the random selection for the train inputs caused a scenario where the propability of the speakers to actually be the same, was really low, thus affecting the training and evaluation of the Models. This could be easily fixed by adding more scenarios, where the speakeakers were same, but for this project, the randomly selecting training samples was instructed. Instead slight adjustments were done by manipulating weights based on the occurrance rates of same / not same speakers.

> Even tho, that Models learned well, and the loss was reduces for both validation and training data, the accuracy was high mainly due to reasons listed in the section above. So in conclusion, the end result is Models, which are most likely not goign to be able to verify the speaker, but are more likely to verify, that the speaker is not who he claims to be based on audio sample.

> Also, because of the validation split, the validation accuracy could be higher and validation loss lower than actual training accuracy or training loss. This is due to factor, that if the validation data had the true values in it (cases where the speakers were actually same), those scenarios would cause more shifting in the weights than the scenarios, where validation data did not had any occurrancers of same speakers.

> The test setup was nice and lots of conclusion could have been draw, in which way the training and evaluation of the Models shoudl have / and should not have been done. In further research it would be practical to create a scenario, where all of the Models are trained on equal number of samples for each scenario (same speaker / not same speaker) and see how the Models perform then by themselves on the similar test data, that was used in this project, and also how they would then perform with other scenarios test data and compare the results to this project. Hypothesis being, that they would outperform Models presented in this project.

## Baseline method description

Description of the baseline method for comparison if you have one (this is a bonus point)

## Results and discussion

Describe the metrics you used to evaluate your algorithms (methods)

>Tensorflow Keras Model.predict and Model.Evaluate were used to provide  information about Models performance on data.

>DET-curves were used, but their effectivenes was not really good, due to random sampling related issues, where the truth values from the evaluations, were pretty much homogenic (all scenarios, where speakers were different).
Sampling option was provided to present some results, which yielded only the fact that usually the models were pretty bad at predicting correct values, if the data was not homogenous (even the Models trained with more equally spread data).

How is the performance of the algorithm?

Good? Why it is good?

>Algorithm seems to be learning to at least predict well on homogenous data, and the loss values were dropping, which indicates, that the Model was actually learning atleast something. This was not too negative tho, as the setup was more towards providing really heterogenous results as the random sapling yields mostly only similar results (not the same speaker), so the Models performance did not came as a surprise.

Bad? Why it is bad?

>Algorithm does not perform well on more heterogenous data, as the Model of the Neural Network provided for the training, was obviously not optimally build for that, as the main setup was to train with randomized data picking, which yielded very biased results.

What could be good improvements for your methods?

> The results are really biased from most of the cases, as the amount of true values (where speakers are the same) is really low and even the weight manipulation can't solve the issue effectively enough. This affects tremendously on the outcome from the training and thus also to prediction and evaluation. (The sample taking was instructed tho to be done with random selection, which lead to this scenario, so it was not fixed for this particular project in other wasy, than providing a little method to make more equally sampled traning and testing, but there was not enough time for me to actually make it efficient to provide more relevant results). The Models trained with less biased data, did not perfrm very well on the evaluation, with any sets, but seemed to predict a lot of false negatives and/or positives.

> it is also notable that the Model for the Neural Network created, was not optimal for the training of the Models with less biased data, as the validation loss indicates in some cases, that the parameters should have been tweaked (at least the learning rate, seemed to be relatively high and was maybe reason behinf the fluctuation of the loss).

> Doing gridsearch to find more optimized parameters for the Model and understanding better the functionalities behind the Models behavior would be beneficial.

> Finding and implementing more accurate evalution metrics would be also helpful.

>Also, as a newcomer to the field, trying methods on different scenario, where the setup would first be more basic, in order to get a hang on the methods better, before diving to the deep end would also be sufficient.

Here is the specific test module for the basic scenario. For some reason seems to crash the runtime so was not useful (probably due to too large set of data for the non-target scores). The basic set can be tested with the normal Test Module it needed.
"""

# Test basic scenario siamese Model agains target and non-target scores.
def test_basic_scenario_siamese(siamese_model, test_samples, verbose:bool = False):

    # MFCCs for each sample/speaker in Test Set.
    test_mfccs = test_samples["mfcc"]

    # Generate Target scores, 10 x MFCCs from utterances from same speaker for each speaker.
    target_scores = []
    for i in range(len(test_samples)):
        speaker = test_samples["spk"].iloc[i]
        mfccs_speaker = test_samples.loc[test_samples["spk"] == speaker, "mfcc"]
        mfccs_target = random.sample(list(mfccs_speaker), 10)
        for value in mfccs_target:
          target_scores.append(value)

    # Generate Non-target scores 50 x MFCCs from utterances from different speaker for each speaker.
    non_target_scores = []
    for i in range(len(test_samples)):
        speaker = test_samples["spk"].iloc[i]
        mfccs_not_speaker = test_samples.loc[test_samples["spk"] != speaker, "mfcc"]
        mfccs_non_target = random.sample(list(mfccs_not_speaker), 50)
        for value in mfccs_non_target:
          non_target_scores.append(mfccs_non_target)

    # Take MFCCs for Target scores.
    mfccs_targets = np.array(target_scores)
    print(mfccs_targets.shape)

    # Take MFCCs for Non-target scores.
    mfccs_non_targets = np.array(non_target_scores)
    print(mfccs_non_targets.shape)

    # Generate correct input format for NNW for Target scores.
    mfccs_input_targets = np.zeros((mfccs_targets.shape[0], 60, 13))
    for i in range(len(mfccs_targets)):
        mfccs_input_targets[i, :] = mfccs_targets[i]

    # Generate correct input format for NNW for Non-target scores.
    mfccs_input_non_targets = np.zeros((mfccs_nontargets.shape[0], 60, 13))
    for i in range(len(mfccs_nontargets)):
        mfccs_input_non_targets[i, :] = mfccs_nontargets[i]

    # Sanity checks.
    print(mfccs_input_targets.shape)
    print(mfccs_input_non_targets.shape)

    # Reshape.
    mfccs_input_targets = mfccs_input_targets.reshape((-1, 60, 13, 1))
    mfccs_input_non_targets = mfccs_input_non_targets.reshape((-1, 60, 13, 1))

    # Evaluate on loop.
    counter_target = 0
    counter_non_target = 0

    # Y true values.
    y_true_target = np.ones(len(test_mfccs))
    y_true_non_target = np.zeros(len(test_mfccs))

    # Iterate over all MFCC samples in 
    for i in range(len(test_mfccs)):
        
        # Predict with model.
        prediction = siamese_model.predict([test_mfccs[i], mfccs_input_targets[counter_target:counter_target+10]])
        print(prediction)     

        # Evaluation score with Target-scores.
        eval_score = siamese_model.evaluate([test_mfccs[i], mfccs_input_targets[counter_target:counter_target+10]], 
                                            y=[mfccs_input_targets[counter_target:counter_target+10]], verbose=2)
        print(eval_score)

        counter_target += 10

        # Predict with model.
        prediction = siamese_model.predict([test_mfccs[i], mfccs_input_non_targets[counter_non_target:counter_non_target +50]])
        print(prediction)

        # Evaluation score with Non-target-scores.
        eval_score = siamese_model.evaluate([test_mfccs[i], mfccs_input_non_targets[counter_non_target:counter_non_target +50]], 
                                            y=[mfccs_input_non_targets[counter_non_target:counter_non_target +50]], verbose=2)
        print(eval_score)

        counter_non_target += 50

# Test the basic scenario.
test_basic_scenario_siamese(siamese_model_basic, test_set_basic)

"""Here is the Test Module for the 3 other scenarios. The DET-curves are not really usable for the 3 scenarios without modifying the test-scenario fist, as the randomly testing yields almost always only same outcome result (not same speaker) which causes the DET-curves to be always blank as the prediction becomes too easy and there are no false negatives or false positives. Use sampling and verbose to modify sampling and DET-curve creation to better visualize the amount of false-negatives and false positives when the data is not so biased."""

# Test given Siamese Model with given Test Samples.
def test_siamese(siamese_model, test_samples, verbose:bool = False, sampling:bool = False):

    # Define array to take scores and y-scores for DET-curve plotting.
    y_pred_scores = []
    y_true_scores = []

    # Get indices from Test Set.
    indices = np.arange(len(test_samples))
    np.random.shuffle(indices)
    indices_clone = np.copy(indices)
    np.random.shuffle(indices_clone)

    # Generate DataFrame based on indices.
    pairwise_map = pd.DataFrame({"sample1": indices, "sample2": indices_clone})

    # If sampling is false, do random picking.
    if sampling == False:
        # Take speakers from Test Set.
        speaker_sample1 = test_samples["spk"].iloc[pairwise_map["sample1"].values].values
        speaker_sample2 = test_samples["spk"].iloc[pairwise_map["sample2"].values].values

        # Check if same speaker.
        y = np.equal(speaker_sample1, speaker_sample2)
        y_true_scores.append(y)

        # Take MFCCs for Test Set.
        mfccs_1 = test_samples["mfcc"].iloc[speaker_sample1].values
        mfccs_2 = test_samples["mfcc"].iloc[speaker_sample2].values

        # Generate correct input format for NNW.
        mfccs_input_1 = np.zeros((len(test_samples), 60, 13))
        mfccs_input_2 = np.zeros((len(test_samples), 60, 13))
        for i in range(len(test_samples)):
            mfccs_input_1[i, :] = mfccs_1[i]
            mfccs_input_2[i, :] = mfccs_2[i]
    
    # if sampling is True, do more equal sampling for providing DET-curves.
    if sampling == True:

        # Take speakers from Test Set.
        speaker_sample1 = test_samples["spk"].iloc[pairwise_map["sample1"].values].values
        speaker_sample2 = []

        # Take MFCCs for Sample Set.
        mfccs_1 = test_samples["mfcc"].iloc[speaker_sample1].values

        # Generate Target-scores MFCC scores for half of the utterances from same speaker for each speaker in first MFCC set.
        mfccs_2_target = []
        for i in range(len(speaker_sample1)//2):
            speaker = speaker_sample1[i]
            speaker_sample2.append(speaker)
            mfccs_speaker = test_samples.loc[test_samples["spk"] == speaker, "mfcc"]
            mfccs_target = random.sample(list(mfccs_speaker), 1)
            mfccs_2_target.append(mfccs_target)
        
        # Generate Non-target scores for the other half.
        mfccs_2_non_target = []
        for i in range(len(speaker_sample1)//2):
            speaker = speaker_sample1[i]
            not_speaker = test_samples.loc[test_samples["spk"] != speaker, "spk"].iloc[0]
            speaker_sample2.append(not_speaker)
            mfccs_not_speaker = test_samples.loc[test_samples["spk"] == not_speaker, "mfcc"]
            mfccs_non_target = random.sample(list(mfccs_not_speaker), 1)
            mfccs_2_non_target.append(mfccs_non_target)

        # Convert to numpy.
        speaker_sample2 = np.array(speaker_sample2)

        # Check if same speaker.
        y = np.equal(speaker_sample1, speaker_sample2)
        y_true_scores.append(y)

        # Combine lists.
        mfccs_2 = mfccs_2_target + mfccs_2_non_target
        mfccs_2 = np.array(mfccs_2)
        
        # Generate correct input format for NNW.
        mfccs_input_1 = np.zeros((len(test_samples), 60, 13))
        mfccs_input_2 = np.zeros((len(test_samples), 60, 13))
        for i in range(len(test_samples)):
            mfccs_input_1[i,:] = mfccs_1[i]
            mfccs_input_2[i,:] = mfccs_2[i]

        
    # Sanity checks.
    print(mfccs_input_1.shape)
    print(mfccs_input_2.shape)

    # Reshape.
    mfccs_input_1 = mfccs_input_1.reshape((-1, 60, 13, 1))
    mfccs_input_2 = mfccs_input_2.reshape((-1, 60, 13, 1))

    # Predict with model.
    print("Predicting with model on Test Set:")
    prediction = siamese_model.predict([mfccs_input_1, mfccs_input_2])
    print(prediction)
    y_pred_scores.append(prediction)

    # Evaluate model with Test Set.
    print("Evaluating the neural network on Test Set:")
    score = siamese_model.evaluate([mfccs_input_1, mfccs_input_2], y=y, verbose=2)
    print(score)

    # Get and plot DET-curves.
    if verbose == True:

      # Convert to NumPy and transpose to use with DET-curve.
      y_pred_scores = np.array(y_pred_scores)
      y_pred_scores = y_pred_scores.reshape((y_pred_scores.shape[0], y_pred_scores.shape[1]))
      y_true_scores = np.array(y_true_scores)

      y_pred_scores = y_pred_scores.T
      y_true_scores = y_true_scores.T

      # Predicted.
      fprate_pred, fnrate_pred, thresholds_pred = det_curve(y_true=y_true_scores, y_score=y_pred_scores, pos_label=None)
      plt.figure()
      display = DetCurveDisplay(
        fpr=fprate_pred, fnr=fnrate_pred, estimator_name='Prediction estimator')
      display.plot()
      plt.show()

"""Test the 3 specific different Models. Set verbose = True, to get DET-curves if any plot-able data is provided (not in most cases) and sampling = True to use sampling, to provide more evenly spread labels:"""

test_siamese(siamese_model_dependent, test_set_dependent, verbose=False, sampling=False)

test_siamese(siamese_model_limited, test_set_limited, verbose=False, sampling=False)

test_siamese(siamese_model_independent, test_set_independent, verbose=False, sampling=False)

test_siamese(siamese_model_dependent_equal, test_set_dependent, verbose=True, sampling=True)

test_siamese(siamese_model_limited_equal, test_set_limited, verbose=True, sampling=True)

test_siamese(siamese_model_independent_equal, test_set_independent, verbose=True, sampling=True)

"""Test Models with data from other Test Sets."""

# Context dependent with others.
test_siamese(siamese_model_dependent, test_set_basic, verbose=False, sampling=False)
test_siamese(siamese_model_dependent, test_set_limited,verbose=False, sampling=False)
test_siamese(siamese_model_dependent, test_set_independent, verbose=False, sampling=False)

test_siamese(siamese_model_dependent, test_set_basic, verbose=False, sampling=True)
test_siamese(siamese_model_dependent, test_set_limited,verbose=False, sampling=True)
test_siamese(siamese_model_dependent, test_set_independent, verbose=False, sampling=True)

# Context limited with others.
test_siamese(siamese_model_limited, test_set_basic, verbose=False, sampling=False)
test_siamese(siamese_model_limited, test_set_dependent, verbose=False, sampling=False)
test_siamese(siamese_model_limited, test_set_independent, verbose=False, sampling=False)

test_siamese(siamese_model_limited, test_set_basic, verbose=False, sampling=True)
test_siamese(siamese_model_limited, test_set_dependent, verbose=False, sampling=True)
test_siamese(siamese_model_limited, test_set_independent, verbose=False, sampling=True)

# Context independent with others.
test_siamese(siamese_model_independent, test_set_basic, verbose=False, sampling=False)
test_siamese(siamese_model_independent, test_set_dependent, verbose=False, sampling=False)
test_siamese(siamese_model_independent, test_set_limited, verbose=False, sampling=False)

test_siamese(siamese_model_independent, test_set_basic, verbose=False, sampling=True)
test_siamese(siamese_model_independent, test_set_dependent, verbose=False, sampling=True)
test_siamese(siamese_model_independent, test_set_limited, verbose=False, sampling=True)

"""Test Models with Models trained with more equally distributed data."""

# Context dependent with others.
test_siamese(siamese_model_dependent_equal, test_set_basic, verbose=False, sampling=False)
test_siamese(siamese_model_dependent_equal, test_set_limited,verbose=False, sampling=False)
test_siamese(siamese_model_dependent_equal, test_set_independent, verbose=False, sampling=False)

test_siamese(siamese_model_dependent_equal, test_set_basic, verbose=False, sampling=True)
test_siamese(siamese_model_dependent_equal, test_set_limited,verbose=False, sampling=True)
test_siamese(siamese_model_dependent_equal, test_set_independent, verbose=False, sampling=True)

# Context limited with others.
test_siamese(siamese_model_limited_equal, test_set_basic, verbose=False, sampling=False)
test_siamese(siamese_model_limited_equal, test_set_dependent, verbose=False, sampling=False)
test_siamese(siamese_model_limited_equal, test_set_independent, verbose=False, sampling=False)

test_siamese(siamese_model_limited_equal, test_set_basic, verbose=False, sampling=True)
test_siamese(siamese_model_limited_equal, test_set_dependent, verbose=False, sampling=True)
test_siamese(siamese_model_limited_equal, test_set_independent, verbose=False, sampling=True)

# Context independent with others.
test_siamese(siamese_model_independent_equal, test_set_basic, verbose=False, sampling=False)
test_siamese(siamese_model_independent_equal, test_set_dependent, verbose=False, sampling=False)
test_siamese(siamese_model_independent_equal, test_set_limited, verbose=False, sampling=False)

test_siamese(siamese_model_independent_equal, test_set_basic, verbose=False, sampling=True)
test_siamese(siamese_model_independent_equal, test_set_dependent, verbose=False, sampling=True)
test_siamese(siamese_model_independent_equal, test_set_limited, verbose=False, sampling=True)

"""In conclusion, there is still lot to do and lots of problemns to be solved. The training of Siamese Neural Network for the first time has proven to have many challenges, which can be definitely overcome in future.

The training of the Models and the scenarios for this project were interesting, yet not the most "easy", when thinking about how much there was to be learned in order to understand why the scenarios worked as they did and why I could not expect really good results from the Models learning or prediction wise.

All in all, the project was very good learning experience and it was quite shame some of the ""little things" in data handling and srugling a bit with the shapes of inputs etc. took a lot of time (as I have not been too familiar with the concept before), thus reducing the amount of time spended with the actual NNW part of the work (as it would have been nice to make them actually work better).

This project was quite inspiring (even tho pretty hard), but in order to get more accurate and relatable Models for Automatic Speaker Verification a lot has to be done to improve the setup made in this project so far.
"""